{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3BTibU1C9kJv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "8991ab54-f8a1-45b0-bfbe-aa9ce2a46c32"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/ERA5_Point 1.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3701585878.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Read the tab-delimited file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_point1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/ERA5_Point 1.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_point2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/ERA5_Point 2.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_point3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/ERA5_Point 3.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/ERA5_Point 1.txt'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# Read the tab-delimited file\n",
        "df_point1 = pd.read_csv('/content/ERA5_Point 1.txt', delimiter='\\t')\n",
        "df_point2 = pd.read_csv('/content/ERA5_Point 2.txt', delimiter='\\t')\n",
        "df_point3 = pd.read_csv('/content/ERA5_Point 3.txt', delimiter='\\t')\n",
        "df_point4 = pd.read_csv('/content/ERA5_Point 4.txt', delimiter='\\t')\n",
        "\n",
        "# Change the time and data to the correct type\n",
        "df_point1['Date/time [UTC]'] = pd.to_datetime(df_point1['Date/time [UTC]'])\n",
        "df_point2['Date/time [UTC]'] = pd.to_datetime(df_point2['Date/time [UTC]'])\n",
        "df_point3['Date/time [UTC]'] = pd.to_datetime(df_point3['Date/time [UTC]'])\n",
        "df_point4['Date/time [UTC]'] = pd.to_datetime(df_point4['Date/time [UTC]'])\n",
        "\n",
        "# Splitting data into training and test\n",
        "# Training years MUST be prior to test years\n",
        "training_years = [2005, 2006, 2007, 2008]\n",
        "test_years = [2011, 2012, 2013, 2014]\n",
        "# Training data\n",
        "train_1 = df_point1[df_point1['Date/time [UTC]'].dt.year.isin(training_years)]\n",
        "train_2 = df_point2[df_point2['Date/time [UTC]'].dt.year.isin(training_years)].drop(columns = \"Date/time [UTC]\")\n",
        "train_3 = df_point3[df_point3['Date/time [UTC]'].dt.year.isin(training_years)].drop(columns = \"Date/time [UTC]\")\n",
        "train_4 = df_point4[df_point4['Date/time [UTC]'].dt.year.isin(training_years)].drop(columns = \"Date/time [UTC]\")\n",
        "\n",
        "# Test Data\n",
        "test_1 = df_point1[df_point1['Date/time [UTC]'].dt.year.isin(test_years)]\n",
        "test_2 = df_point2[df_point2['Date/time [UTC]'].dt.year.isin(test_years)].drop(columns = \"Date/time [UTC]\")\n",
        "test_3 = df_point3[df_point3['Date/time [UTC]'].dt.year.isin(test_years)].drop(columns = \"Date/time [UTC]\")\n",
        "test_4 = df_point4[df_point4['Date/time [UTC]'].dt.year.isin(test_years)].drop(columns = \"Date/time [UTC]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-dCI5nqCD5J"
      },
      "outputs": [],
      "source": [
        "# Concat all data to one df\n",
        "train_data_total = pd.concat([train_1, train_2, train_3, train_4], axis=1)\n",
        "# Get all DateTime Info\n",
        "time_data = pd.to_datetime(train_data_total['Date/time [UTC]'])\n",
        "# Extract useful date features\n",
        "X_time_features = pd.DataFrame({\n",
        "    \"year\": time_data.dt.year,\n",
        "    \"month\": time_data.dt.month,\n",
        "    \"day_of_year\": time_data.dt.dayofyear,\n",
        "    \"day_of_week\": time_data.dt.weekday,  # 0=Monday, 6=Sunday\n",
        "    \"hour\": time_data.dt.hour,\n",
        "    \"minute\": time_data.dt.minute})\n",
        "# Add all these columns to our df and remove the DateTime values\n",
        "# train_data_total = pd.concat([train_data_total, X_time_features], axis=1)\n",
        "train_data_total = train_data_total.drop(columns=['Date/time [UTC]'])\n",
        "# Checking for the shape fo the new data\n",
        "print(f\"Data is {train_data_total.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2GyFOGKC0zm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class WeatherDataset(Dataset):\n",
        "    def __init__(self, data, temporal_features, normalize=True):\n",
        "        \"\"\"\n",
        "        Custom Dataset for weather data with advanced normalization\n",
        "        \"\"\"\n",
        "        self.data = torch.FloatTensor(data)\n",
        "        self.temporal_features = torch.FloatTensor(temporal_features)\n",
        "\n",
        "        if normalize:\n",
        "            # Per-feature normalization with robust scaling\n",
        "            self.data_median = self.data.median(dim=0).values\n",
        "            self.data_iqr = (self.data.quantile(0.75, dim=0) -\n",
        "                              self.data.quantile(0.25, dim=0))\n",
        "\n",
        "            # Robust normalization to handle outliers\n",
        "            self.normalized_data = (self.data - self.data_median) / (self.data_iqr + 1e-7)\n",
        "        else:\n",
        "            self.normalized_data = self.data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            self.normalized_data[idx],\n",
        "            self.temporal_features[idx]\n",
        "        )\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    \"\"\"Swish activation function\"\"\"\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block for deeper networks\"\"\"\n",
        "    def __init__(self, in_features):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Linear(in_features, in_features),\n",
        "            nn.BatchNorm1d(in_features),\n",
        "            Swish(),\n",
        "            nn.Linear(in_features, in_features),\n",
        "            nn.BatchNorm1d(in_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "class WeatherVAE(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim=16,\n",
        "                 temporal_dim=6,\n",
        "                 hidden_dims=[64, 32],\n",
        "                 latent_dim=10,\n",
        "                 num_residual_blocks=2):\n",
        "        super(WeatherVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        # Combine weather and temporal features\n",
        "        combined_dim = input_dim + temporal_dim\n",
        "\n",
        "        # Encoder with residual connections\n",
        "        encoder_layers = []\n",
        "        prev_dim = combined_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            encoder_layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                Swish(),\n",
        "                *[ResidualBlock(hidden_dim) for _ in range(num_residual_blocks)]\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "        # Latent space layers with improved initialization\n",
        "        self.fc_mu = nn.Linear(prev_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(prev_dim, latent_dim)\n",
        "\n",
        "        # Initialize latent space layers with small weights\n",
        "        nn.init.xavier_normal_(self.fc_mu.weight, gain=0.01)\n",
        "        nn.init.xavier_normal_(self.fc_logvar.weight, gain=0.01)\n",
        "\n",
        "        # Decoder with residual connections\n",
        "        decoder_layers = []\n",
        "        prev_dim = latent_dim + temporal_dim\n",
        "        for hidden_dim in reversed(hidden_dims):\n",
        "            decoder_layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                Swish(),\n",
        "                *[ResidualBlock(hidden_dim) for _ in range(num_residual_blocks)]\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "    def encode(self, x, temporal):\n",
        "        \"\"\"Encode input to latent distribution\"\"\"\n",
        "        combined = torch.cat([x, temporal], dim=1)\n",
        "        h = self.encoder(combined)\n",
        "        return self.fc_mu(h), self.fc_logvar(h)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        \"\"\"Reparameterization trick with clipping\"\"\"\n",
        "        std = torch.exp(0.5 * torch.clamp(logvar, min=-10, max=2))\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z, temporal):\n",
        "        \"\"\"Decode latent representation\"\"\"\n",
        "        combined = torch.cat([z, temporal], dim=1)\n",
        "        return self.decoder(combined)\n",
        "\n",
        "    def forward(self, x, temporal):\n",
        "        \"\"\"Forward pass\"\"\"\n",
        "        mu, logvar = self.encode(x, temporal)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z, temporal), mu, logvar\n",
        "\n",
        "    def sample(self, temporal, num_samples=1):\n",
        "        \"\"\"\n",
        "        Sample from the latent space\n",
        "        \"\"\"\n",
        "        temporal = temporal.repeat(num_samples, 1)\n",
        "        z = torch.randn(num_samples, self.latent_dim, device=temporal.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            samples = self.decode(z, temporal)\n",
        "\n",
        "        return samples\n",
        "\n",
        "def advanced_vae_loss(recon_x, x, mu, logvar, beta=1.0):\n",
        "    \"\"\"\n",
        "    Advanced VAE Loss Function\n",
        "    - Reconstruction Loss (Huber Loss for robustness)\n",
        "    - KL Divergence with adaptive beta\n",
        "    - Gradient clipping\n",
        "    \"\"\"\n",
        "    # Huber loss for more robust reconstruction\n",
        "    recon_loss = F.smooth_l1_loss(recon_x, x, reduction='sum')\n",
        "\n",
        "    # KL Divergence loss with adaptive scaling\n",
        "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    # Adaptive beta based on reconstruction loss\n",
        "    adaptive_beta = beta * (recon_loss.item() / (x.numel() + 1e-8))\n",
        "\n",
        "    return recon_loss + adaptive_beta * kl_loss\n",
        "\n",
        "def train_vae(model, dataloader, optimizer, device, epochs=50,\n",
        "              early_stopping_patience=10):\n",
        "    \"\"\"\n",
        "    Enhanced training loop with early stopping and learning rate scheduling\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_features, batch_temporal in dataloader:\n",
        "            batch_features = batch_features.to(device)\n",
        "            batch_temporal = batch_temporal.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            recon_batch, mu, logvar = model(batch_features, batch_temporal)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = advanced_vae_loss(recon_batch, batch_features, mu, logvar)\n",
        "\n",
        "            # Backward pass with gradient clipping\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Average loss and learning rate scheduling\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        scheduler.step(avg_loss)\n",
        "\n",
        "        # Early stopping\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')\n",
        "\n",
        "        # Stop if no improvement\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCCgJt9jC2yY",
        "outputId": "049fa4c6-77fb-4bb6-8471-4f0ecdac6b69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Loss: 144.3283\n",
            "Epoch [2/50], Loss: 143.2419\n",
            "Epoch [3/50], Loss: 140.2427\n",
            "Epoch [4/50], Loss: 135.9487\n",
            "Epoch [5/50], Loss: 132.9676\n",
            "Epoch [6/50], Loss: 132.3508\n",
            "Epoch [7/50], Loss: 131.3245\n",
            "Epoch [8/50], Loss: 130.1719\n",
            "Epoch [9/50], Loss: 128.9714\n",
            "Epoch [10/50], Loss: 128.6170\n",
            "Epoch [11/50], Loss: 127.9933\n",
            "Epoch [12/50], Loss: 127.0288\n",
            "Epoch [13/50], Loss: 125.9158\n",
            "Epoch [14/50], Loss: 125.0675\n",
            "Epoch [15/50], Loss: 124.7369\n",
            "Epoch [16/50], Loss: 124.8794\n",
            "Epoch [17/50], Loss: 124.2744\n",
            "Epoch [18/50], Loss: 124.3570\n",
            "Epoch [19/50], Loss: 123.3692\n",
            "Epoch [20/50], Loss: 123.0800\n",
            "Epoch [21/50], Loss: 121.3355\n",
            "Epoch [22/50], Loss: 121.1839\n",
            "Epoch [23/50], Loss: 122.6173\n",
            "Epoch [24/50], Loss: 121.0485\n",
            "Epoch [25/50], Loss: 120.9201\n",
            "Epoch [26/50], Loss: 120.6899\n",
            "Epoch [27/50], Loss: 119.5133\n",
            "Epoch [28/50], Loss: 120.1273\n",
            "Epoch [29/50], Loss: 121.0304\n",
            "Epoch [30/50], Loss: 121.2323\n",
            "Epoch [31/50], Loss: 121.1112\n",
            "Epoch [32/50], Loss: 117.7377\n",
            "Epoch [33/50], Loss: 115.3766\n",
            "Epoch [34/50], Loss: 112.5921\n",
            "Epoch [35/50], Loss: 113.9735\n",
            "Epoch [36/50], Loss: 114.1664\n",
            "Epoch [37/50], Loss: 112.8282\n",
            "Epoch [38/50], Loss: 112.1688\n",
            "Epoch [39/50], Loss: 110.8529\n",
            "Epoch [40/50], Loss: 109.2072\n",
            "Epoch [41/50], Loss: 107.4571\n",
            "Epoch [42/50], Loss: 107.8378\n",
            "Epoch [43/50], Loss: 106.4665\n",
            "Epoch [44/50], Loss: 105.1032\n",
            "Epoch [45/50], Loss: 104.6579\n",
            "Epoch [46/50], Loss: 103.3671\n",
            "Epoch [47/50], Loss: 102.7790\n",
            "Epoch [48/50], Loss: 102.7992\n",
            "Epoch [49/50], Loss: 100.9930\n",
            "Epoch [50/50], Loss: 100.1791\n"
          ]
        }
      ],
      "source": [
        "# Example usage (you would replace this with your actual data)\n",
        "def main():\n",
        "    # Simulated data (replace with your actual weather dataset)\n",
        "    weather_data = np.array(train_data_total) # 1000 samples, 16 features\n",
        "    temporal_data = np.array(X_time_features)  # 6 temporal features\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = WeatherDataset(weather_data, temporal_data)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Initialize model\n",
        "    model = WeatherVAE()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Train model\n",
        "    trained_model = train_vae(model, dataloader, optimizer,\n",
        "                               device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "    return trained_model, generated_samples\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, samples = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-Ww2M3oE9ll"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def filter_data_by_datetime(df, datetime_col, year=None, month=None, day_of_year=None,\n",
        "                            day_of_week=None, hour=None, minute=None):\n",
        "    \"\"\"\n",
        "    Filters a DataFrame based on specific datetime components.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        datetime_col (str): The name of the column with datetime values.\n",
        "        year (int, optional): The year to filter by.\n",
        "        month (int, optional): The month to filter by (1-12).\n",
        "        day_of_year (int, optional): The day of the year to filter by (1-366).\n",
        "        day_of_week (int, optional): The day of the week to filter by (0=Monday, 6=Sunday).\n",
        "        hour (int, optional): The hour to filter by (0-23).\n",
        "        minute (int, optional): The minute to filter by (0-59).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Filtered DataFrame.\n",
        "    \"\"\"\n",
        "    df = df.copy()  # Avoid modifying the original DataFrame\n",
        "    df[datetime_col] = pd.to_datetime(df[datetime_col])  # Ensure datetime format\n",
        "\n",
        "    # Filter based on conditions\n",
        "    mask = pd.Series(True, index=df.index)  # Start with all True\n",
        "\n",
        "    if year is not None:\n",
        "        mask &= df[datetime_col].dt.year == year\n",
        "    if month is not None:\n",
        "        mask &= df[datetime_col].dt.month == month\n",
        "    if day_of_year is not None:\n",
        "        mask &= df[datetime_col].dt.dayofyear == day_of_year\n",
        "    if day_of_week is not None:\n",
        "        mask &= df[datetime_col].dt.dayofweek == day_of_week\n",
        "    if hour is not None:\n",
        "        mask &= df[datetime_col].dt.hour == hour\n",
        "    if minute is not None:\n",
        "        mask &= df[datetime_col].dt.minute == minute\n",
        "\n",
        "    return df[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "PBrHF9FJExFD",
        "outputId": "df462a10-4865-4be6-ce5e-d115e838a222"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'WeatherVAE' object has no attribute 'latent_dim'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-9376cd8a4dff>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msample_temporal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2009\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgenerated_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_temporal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_point1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_point1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date/time [UTC]'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2009\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-111f2d72532b>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, temporal, num_samples)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \"\"\"\n\u001b[1;32m    132\u001b[0m         \u001b[0mtemporal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemporal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemporal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1932\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'WeatherVAE' object has no attribute 'latent_dim'"
          ]
        }
      ],
      "source": [
        "  # Example sampling\n",
        "sample_temporal = torch.tensor([[2009, 1, 1, 0, 6, 0]], dtype=torch.float32)\n",
        "generated_samples = model.sample(sample_temporal, num_samples=5)\n",
        "\n",
        "test = df_point1[df_point1['Date/time [UTC]'].dt.year.isin([2009])]\n",
        "\n",
        "actual_data = filter_data_by_datetime(\n",
        "                                      test,\n",
        "                                      datetime_col='Date/time [UTC]',\n",
        "                                      year=2009,\n",
        "                                      month=1,\n",
        "                                      day_of_year = 1,\n",
        "                                      hour = 6,\n",
        "                                      minute = 0)\n",
        "\n",
        "print(f\"Actual Data: {actual_data}\")\n",
        "print(generated_samples[:][0:4])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}